import json
import os
import pickle
import re
import tempfile
from collections import Counter
from datetime import datetime

import tiktoken
from django.contrib.auth.mixins import LoginRequiredMixin
from django.db import NotSupportedError
from django.http import HttpResponse, JsonResponse, HttpResponseBadRequest
from django.template.loader import render_to_string
from django.utils.timezone import make_aware
from django.views import View
from django.db.models import Subquery, OuterRef, Q, Prefetch, Count
from django.db.models.functions import Length
from django.http import StreamingHttpResponse
from django.shortcuts import render, redirect, get_object_or_404
from django.urls import reverse_lazy
from django.views.generic import ListView, DetailView

from langchain_core.documents import Document
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

from app_chunks.forms import ModelScoreTestForm, SplitterSelectForm, SplitterDynamicConfigForm
from app_chunks.models import Chunk, ChunkStatus
from app_chunks.splitters.dispatcher import SplitterDispatcher
from app_chunks.tasks import test_model_answer
from app_embeddings.services.embedding_config import system_instruction
from app_embeddings.services.retrieval_engine import answer_index
from app_sources.content_models import URLContent
from app_sources.source_models import URL, SourceStatus
from app_sources.storage_models import WebSite
from utils.setup_logger import setup_logger

logger = setup_logger(__name__, log_dir="logs/chunking", log_file="chunking.log")


class SplitterConfigView(LoginRequiredMixin, View):
    """–í—å—é –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞ –ø–æ –∫–ª–∞—Å—Å—É"""

    def get(self, request, *args, **kwargs):
        splitter_class_name = request.GET.get("splitter_class_name")
        if not splitter_class_name:
            return HttpResponseBadRequest("–ü–∞—Ä–∞–º–µ—Ç—Ä 'splitter_class_name' –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω")

        try:
            dispatcher = SplitterDispatcher()
            splitter_cls = dispatcher.get_by_name(splitter_class_name)
        except ValueError:
            return HttpResponseBadRequest("–°–ø–ª–∏—Ç—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω")

        config_schema = getattr(splitter_cls, "config_schema", {})
        config_form = SplitterDynamicConfigForm(schema=config_schema)

        html = render_to_string("widgets/_form_content-widget.html", {
            "form": config_form,
        }, request=request)

        return HttpResponse(html)


def split_markdown_text(markdown_text,
                        strip_headers=False):  # –ù–ï —É–¥–∞–ª—è—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ–¥ '#..' –∏–∑ page_content

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –±—É–¥–µ–º —Ä–∞–∑–±–∏–≤–∞—Ç—å —Ç–µ–∫—Å—Ç
    headers_to_split_on = [("#", "Header 1"),  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è
                           ("##", "Header 2"),  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≤—Ç–æ—Ä–æ–≥–æ —É—Ä–æ–≤–Ω—è
                           ("###", "Header 3")]  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç—Ä–µ—Ç—å–µ–≥–æ —É—Ä–æ–≤–Ω—è
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä MarkdownHeaderTextSplitter —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on,
                                                   strip_headers=strip_headers)
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç LangChain Document
    chunks = markdown_splitter.split_text(markdown_text)
    return chunks  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ LangChain


def num_tokens_from_string(string: str, encoding_name: str) -> int:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç—Ä–æ–∫–µ"""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens


def split_text(text, max_count):
    headers_to_split_on = [
        ("#", "Header 1"),
        # ("##", "Header 2"),
    ]

    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)
    fragments = markdown_splitter.split_text(text)

    # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    fragment_token_counts = [num_tokens_from_string(fragment.page_content, "cl100k_base") for fragment in fragments]
    # plt.hist(fragment_token_counts, bins=50, alpha=0.5, label='Fragments')
    # plt.title('Distribution of Fragment Token Counts')
    # plt.xlabel('Token Count')
    # plt.ylabel('Frequency')
    # plt.show()
    # print(fragment_token_counts)
    # for fragment in fragments:
    #     if num_tokens_from_string(fragment.page_content, "cl100k_base") > max_count:
    #         print(fragment)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_count,
        chunk_overlap=50,
        length_function=lambda x: num_tokens_from_string(x, "cl100k_base")
    )

    source_chunks = [
        Document(page_content=chunk, metadata=fragment.metadata)
        for fragment in fragments
        for chunk in splitter.split_text(fragment.page_content)
    ]

    # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ source_chunk –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    source_chunk_token_counts = [num_tokens_from_string(chunk.page_content, "cl100k_base") for chunk in source_chunks]
    # plt.hist(source_chunk_token_counts, bins=20, alpha=0.5, label='Source Chunks')
    # plt.title('Distribution of Source Chunk Token Counts')
    # plt.xlabel('Token Count')
    # plt.ylabel('Frequency')
    # plt.show()
    # print(source_chunk_token_counts)

    return source_chunks, fragments


def deep_clean_metadata(obj):
    if isinstance(obj, dict):
        return {k: deep_clean_metadata(v) for k, v in obj.items() if deep_clean_metadata(v)}
    elif isinstance(obj, list):
        return [deep_clean_metadata(v) for v in obj if deep_clean_metadata(v)]
    return obj if obj not in (None, "", [], {}) else None


def process_document(doc: Document) -> Document:
    # –ò–∑–≤–ª–µ–∫–∞–µ–º page_content –∏ metadata –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    page_content = doc.page_content
    metadata = doc.metadata.copy()  # –°–æ–∑–¥–∞—ë–º –∫–æ–ø–∏—é metadata
    files = metadata.get('files', {})

    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π –∏ —Å–ø–∏—Å–∫–æ–≤ –≤ files
    def process_files(data, page_content, prefix='files', parent_name=None):
        if isinstance(data, dict):
            for key, value in data.items():
                page_content = process_files(value, page_content, f"{prefix}__{key}", parent_name)
        elif isinstance(data, list):
            for i, item in enumerate(data):
                # –ï—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Å–ø–∏—Å–∫–∞ ‚Äî —Å–ø–∏—Å–æ–∫, –ø–µ—Ä–µ–¥–∞—ë–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–∞–∫ –∏–º—è
                if isinstance(item, list):
                    name = item[0] if len(item) > 0 else None
                    for j, sub_item in enumerate(item):
                        page_content = process_files(sub_item, page_content, f"{prefix}__{i}__{j}", name)
                else:
                    page_content = process_files(item, page_content, f"{prefix}__{i}", parent_name)
        elif isinstance(data, str) and 'http' in data:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å—Å—ã–ª–∫–æ–π
            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –≤ page_content –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown –∏–ª–∏ –∫–∞–∫ —á–∏—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
            markdown_pattern = rf'!\[(.*?)\]\({re.escape(data)}\)'
            match = re.search(markdown_pattern, page_content)
            if match or data in page_content:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º parent_name –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ Markdown –∏–ª–∏ —Å—Å—ã–ª–∫—É
                name = parent_name if parent_name else (match.group(1) if match else data)
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ metadata –Ω–∞ –∫–æ—Ä–Ω–µ–≤–æ–π —É—Ä–æ–≤–µ–Ω—å
                metadata[prefix] = f"{name} {data}"
                # –ó–∞–º–µ–Ω—è–µ–º Markdown-—Å—Å—ã–ª–∫—É –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ —á–∏—Å—Ç—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –∏–º—è
                if match:
                    page_content = re.sub(markdown_pattern, match.group(1), page_content)
                else:
                    page_content = page_content.replace(data, name)
        return page_content

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ files, –æ–±–Ω–æ–≤–ª—è—è page_content
    page_content = process_files(files, page_content)

    # –£–¥–∞–ª—è–µ—Ç Markdown-–∑–∞–≥–æ–ª–æ–≤–∫–∏ (###, ##, #) –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ —Å—Ç—Ä–æ–∫–∏
    page_content = re.sub(r'\s*#{1,6}\s*', ' ', page_content)
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø–æ—Å–ª–µ –∑–∞–º–µ–Ω—ã Markdown-—Å—Å—ã–ª–æ–∫
    page_content = re.sub(r'\s+', ' ', page_content).strip()

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º files –≤ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
    metadata['files'] = []
    metadata = deep_clean_metadata(metadata)

    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç Document —Å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    return Document(page_content=page_content, metadata=metadata)


def flatten_metadata(d, parent_key='', sep='_'):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ä–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å/—Å–ø–∏—Å–æ–∫ –≤ –ø–ª–æ—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å.
    –ö–ª—é—á–∏ —Å–æ–µ–¥–∏–Ω—è—é—Ç—Å—è —á–µ—Ä–µ–∑ sep, —Å–ø–∏—Å–∫–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É—é—Ç—Å—è.
    """
    items = []
    if isinstance(d, dict):
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            items.extend(flatten_metadata(v, new_key, sep=sep).items())
    elif isinstance(d, list):
        for i, v in enumerate(d):
            new_key = f"{parent_key}{sep}{i}"
            items.extend(flatten_metadata(v, new_key, sep=sep).items())
    else:
        items.append((parent_key, d))
    return dict(items)


def save_documents_to_file(all_documents, filename):
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã
    file_path = os.path.join(os.pardir, filename)

    # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –±–∏–Ω–∞—Ä–Ω–æ–º —Ä–µ–∂–∏–º–µ
    with open(file_path, 'wb') as f:
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º (—Å–æ—Ö—Ä–∞–Ω—è–µ–º) —Å–ø–∏—Å–æ–∫ all_documents –≤ —Ñ–∞–π–ª
        pickle.dump(all_documents, f)
    print(f"–î–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {file_path}")


def save_documents_to_response(request, documents, is_ajax=False):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –≤ HTTP-–æ—Ç–≤–µ—Ç–µ —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –ø–µ—Ä–µ–¥–∞—á–µ–π."""
    if not documents:
        if is_ajax:
            return JsonResponse({"error": "No documents to save"}, status=400)
        return HttpResponse("No documents to save", status=400)

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pickle',
                                            dir=os.path.dirname(os.path.abspath(__file__)))
    temp_file_path = temp_file.name

    try:
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –∑–∞–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª
        pickle.dump(documents, temp_file)
        temp_file.close()

        if is_ajax:
            # –î–ª—è AJAX: —á–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ Blob —Å JSON
            file_content = temp_file.read()
            response = JsonResponse({
                "status": "success",
                "filename": "chunk.pickle",
                "content_type": "application/octet-stream"
            })
            response['Content-Disposition'] = 'attachment; filename="chunk.pickle"'
            response.content = file_content  # –ü–µ—Ä–µ–¥–∞—ë–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            return response

        def file_iterator(path):
            try:
                with open(path, 'rb') as f:
                    while chunk := f.read(8192):
                        yield chunk
            finally:
                # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–µ—Ä–µ–¥–∞—á–∏
                if os.path.exists(path):
                    os.unlink(path)

        response = StreamingHttpResponse(
            file_iterator(temp_file_path),
            content_type='application/octet-stream'
        )
        response['Content-Disposition'] = 'attachment; filename="chunk.pickle"'
        return response

    except Exception as e:
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Ç–æ–ª—å–∫–æ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if os.path.exists(temp_file_path):
            os.unlink(temp_file_path)
        return HttpResponse(f"Error serializing documents: {str(e)}", status=500)


class ChunkListView(LoginRequiredMixin, ListView):
    """–°–ø–∏—Å–∫–æ–≤—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä —á–∞–Ω–∫–æ–≤"""
    # TODO –ø—Ä–∞–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–ª–∞–¥–µ–ª—å—Ü–∞
    model = Chunk
    queryset = Chunk.objects.select_related("author").annotate()

    def get(self, *args, **kwargs):
        queryset = super().get_queryset()
        url_content_pk = self.request.GET.get("urlcontent")
        if url_content_pk:
            url_content = URLContent.objects.get(pk=url_content_pk)
            document = url_content.url
            storage = document.site
            kb = storage.kb

            queryset = queryset.filter(url_content=url_content)
            context = {
                "content": url_content,
                "content_type_ru": "–ß–∏—Å—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç",
                "document": document,
                "document_type_ru": "–í–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞",
                "storage": storage,
                "storage_type_eng": "website",
                "storage_type_ru": "–í–µ–±-—Å–∞–π—Ç",
                "kb": kb,
                "object": url_content,
                "chunk_list": queryset,
            }

        return render(request=self.request, template_name="app_chunks/chunk_list.html", context=context)


class ChunkDetailView(LoginRequiredMixin, DetailView):
    """–î–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä Chunk"""
    # TODO –ø—Ä–∞–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–ª–∞–¥–µ–ª—å—Ü–∞
    model = Chunk


class ChunkCreateFromURLContentView(LoginRequiredMixin, View):
    """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ URLContent"""

    def get(self, request, url_content_pk, *args, **kwargs):
        chunk_preview_set = Prefetch(
            "chunk_set",
            queryset=Chunk.objects.order_by("pk").only("id", "status", "metadata")[:5],
            to_attr="chunk_preview_set"
        )

        urlcontent_qs = URLContent.objects.select_related("report", "author") \
            .prefetch_related(chunk_preview_set) \
            .annotate(chunks_counter=Count("chunk"))

        url_content = get_object_or_404(urlcontent_qs, pk=url_content_pk)
        document = url_content.url
        storage = document.site
        kb = storage.kb
        context = {
            "kb": kb,
            "storage": storage,
            "storage_type_ru": "–í–µ–±-—Å–∞–π—Ç",
            "storage_type_eng": "website",
            "document": document,
            "document_type_ru": "–í–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞",
            "content": url_content,
            "content_type_ru": "–ß–∏—Å—Ç—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç",
            "object": url_content,
        }
        # —Ñ–æ—Ä–º–∞ –≤—ã–±–æ—Ä–∞ –∫–ª–∞—Å—Å–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è
        dispatcher = SplitterDispatcher()
        splitters = dispatcher.list_all()
        logger.error(splitters)
        splitter_select_form = SplitterSelectForm(splitters=splitters)
        config_form = SplitterDynamicConfigForm()
        context["form"] = splitter_select_form
        context["config_form"] = config_form
        return render(request, "app_chunks/urlcontent_chunking.html", context)

    def post(self, request, url_content_pk, *args, **kwargs):
        url_content = get_object_or_404(URLContent, pk=url_content_pk)
        document = url_content.url
        storage = document.site
        kb = storage.kb
        context = {
            "content": url_content,
            "document": document,
            "storage": storage,
            "kb": kb,
        }

        dispatcher = SplitterDispatcher()
        splitters = dispatcher.list_all()
        splitter_select_form = SplitterSelectForm(request.POST, splitters=splitters)

        if not splitter_select_form.is_valid():
            context["form"] = splitter_select_form
            context["config_form"] = None
            return render(request, "app_chunks/urlcontent_chunking.html", context)
        splitter_cls = splitter_select_form.cleaned_data.get("splitters")
        splitter_config_schema = getattr(splitter_cls, "config_schema", {})
        config_form = SplitterDynamicConfigForm(request.POST, schema=splitter_config_schema)
        if not config_form.is_valid():
            context["form"] = splitter_select_form
            context["config_form"] = config_form
            return render(request, "app_chunks/urlcontent_chunking.html", context)
        splitter_config = config_form.cleaned_data
        splitter = splitter_cls(splitter_config)

        body = url_content.body
        metadata = url_content.metadata

        url = url_content.url.url
        metadata["url"] = url

        documents = splitter.split(metadata=metadata, text_to_split=body)

        save_documents_to_file(documents, "chunk.pickle")
        bulk_container = []
        for document in documents:
            chunk = Chunk(
                url_content=url_content,
                status=ChunkStatus.READY.value,
                metadata=document.metadata,
                page_content=document.page_content,
                splitter_cls=splitter_cls.__name__,
                splitter_config=splitter_config,
                author=request.user,
            )
            bulk_container.append(chunk)
        if bulk_container:
            Chunk.objects.bulk_create(bulk_container)

        return redirect(reverse_lazy("chunks:chunk_list") + f"?urlcontent={url_content.pk}")


class ChunkCreateFromWebSiteView(LoginRequiredMixin, View):
    """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ URLContent"""

    @staticmethod
    def parse_date_param(param):
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ISO-–¥–∞—Ç—É "YYYY-MM-DD" ‚Üí datetime + timezone
            return make_aware(datetime.strptime(param, "%Y-%m-%d"))
        except (ValueError, TypeError):
            return None

    @staticmethod
    def parse_int_param(param):
        try:
            return int(param) if param else None
        except (ValueError, TypeError):
            return None

    def post(self, request, pk):
        website = WebSite.objects.get(pk=pk)
        is_ajax = request.headers.get('X-Requested-With') == 'XMLHttpRequest'

        standard_range_filter = {
            "–¥–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è": {
                "type": "date",
                "pairs": (
                    ("created_at__gte", "—Å"),
                    ("created_at__lte", "–ø–æ"),
                ),
            }
        }
        nonstandard_range_filter = {
            "–¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (—Å–∏–º–≤–æ–ª–æ–≤)": {
                "annotations": {"urlcontent__body_length": Length("urlcontent__body")},
                "type": "number",
                "pairs": (
                    ("urlcontent__body_length__gte", "–æ—Ç"),
                    ("urlcontent__body_length__lte", "–¥–æ"),
                ),
            }
        }

        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ POST
        search_query = request.POST.get("search", "").strip()
        status_filter = request.POST.getlist("status", None)
        tags_filter = request.POST.getlist("tags", None)
        print(search_query)
        print(status_filter)
        print(tags_filter)

        urls = website.url_set.all()

        # üîç –ü–æ–∏—Å–∫
        if search_query:
            urls = urls.filter(
                Q(title__icontains=search_query) | Q(url__icontains=search_query)
            )

        # ‚úÖ –°—Ç–∞—Ç—É—Å—ã
        if status_filter:
            valid_statuses = [status.value for status in SourceStatus]
            filtered_statuses = [s for s in status_filter if s in valid_statuses]
            if filtered_statuses:
                urls = urls.filter(status__in=filtered_statuses)

        # üè∑ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–≥–∞–º
        # –í–µ—Ä—Å–∏—è –¥–ª—è PostgreSQL
        # if tags_filter:
        #     # –§–∏–ª—å—Ç—Ä—É–µ–º URL, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å URLContent —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ —Ç–µ–≥–∞–º–∏
        #     urls = urls.filter(
        #         urlcontent__tags__contains=tags_filter
        #     ).distinct()

        if tags_filter:
            # –ü–æ–ª—É—á–∞–µ–º ID –≤—Å–µ—Ö URLContent, –≥–¥–µ tags —Å–æ–¥–µ—Ä–∂–∞—Ç —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–µ–≥ –∏–∑ tags_filter
            urlcontent_ids = URLContent.objects.filter(
                url__site=website
            ).values_list("id", "tags")
            matching_url_ids = set()
            for uc_id, tags in urlcontent_ids:
                if tags and any(tag in tags for tag in tags_filter):
                    url_id = URLContent.objects.get(id=uc_id).url_id
                    matching_url_ids.add(url_id)
            urls = urls.filter(id__in=matching_url_ids).distinct()

        # üìÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω (–¥–∞—Ç—ã)
        standard_range_query = {}
        for group in standard_range_filter.values():
            for param_key, _ in group["pairs"]:
                raw_value = request.POST.get(param_key)
                if raw_value and raw_value.strip():
                    aware_date = self.parse_date_param(raw_value)
                    if aware_date:
                        standard_range_query[param_key] = aware_date
        if standard_range_query:
            urls = urls.filter(**standard_range_query)

        # üîß –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ + –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω (—á–∏—Å–ª–æ–≤—ã–µ)
        combined_annotations = {}
        nonstandard_range_query = {}
        for item in nonstandard_range_filter.values():
            item_annotations = item.get("annotations", {})
            should_add_annotation = False

            for param_key, _ in item["pairs"]:
                val = request.POST.get(param_key)
                if val is not None and val.strip() != "":
                    try:
                        if item["type"] == "number":
                            int_val = self.parse_int_param(val.strip())
                            if int_val is not None:
                                nonstandard_range_query[param_key] = int_val
                                should_add_annotation = True
                    except ValueError:
                        pass

            if should_add_annotation:
                combined_annotations.update(item_annotations)

        if combined_annotations:
            urls = urls.annotate(**combined_annotations)
        if nonstandard_range_query:
            urls = urls.filter(**nonstandard_range_query)

        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ URLContent –¥–ª—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö URL
        url_content_qs = URLContent.objects.filter(url_id=OuterRef("id")).order_by("-created_at")[:1]
        urls = urls.prefetch_related(
            Prefetch(
                "urlcontent_set",
                queryset=URLContent.objects.filter(id__in=Subquery(url_content_qs.values("id"))),
                to_attr="related_urlcontents"
            )
        ).annotate(
            urlcontent_total_count=Count("urlcontent")
        )

        # –°–æ–±–∏—Ä–∞–µ–º URLContent
        url_contents = []
        for url in urls:
            if url.related_urlcontents:
                url_contents.append(url.related_urlcontents[0])

        print(url_contents)
        print("–î–ª–∏–Ω–∞ —Å–µ—Ç–∞", len(url_contents))
        documents = []
        for url_content in url_contents:
            url = url_content.url.url
            # print(url)
            body = url_content.body
            metadata = url_content.metadata
            metadata.pop("internal_links")

            total_header = ""
            if metadata.get("title"):
                total_header += "–ó–∞–≥–æ–ª–æ–≤–æ–∫: " + metadata.get("title") + ". "
            if metadata.get("tags"):
                total_header += "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: " + ", ".join(metadata.get("tags")) + ". "

            # chunks = split_markdown_text(body)
            source_chunks, fragments = split_text(body, 750)
            for chunk in source_chunks:
                chunk.metadata.update(metadata)
                chunk.metadata["url"] = url

            source_chunks = [process_document(chunk) for chunk in source_chunks]

            for chunk in source_chunks:
                chunk.page_content = total_header + chunk.page_content
                chunk.metadata = flatten_metadata(chunk.metadata)

            documents.extend(source_chunks)

        # save_documents_to_file(documents, "chunk.pickle")
        response = save_documents_to_response(request, documents, is_ajax)
        return response


class TestAskFridaView(LoginRequiredMixin, View):
    template_name = "app_chunks/ai_chat.html"

    def get(self, request, *args, **kwargs):
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞ –∏–∑ —Å–µ—Å—Å–∏–∏ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é
        chat_history = request.session.get('chat_history', [])
        return render(request, self.template_name, {'chat_history': chat_history})

    def post(self, request):
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞ –∏–∑ —Å–µ—Å—Å–∏–∏
        chat_history = request.session.get('chat_history', [])

        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_message = request.POST.get('message', '').strip()

        if user_message:
            docs, ai_message = answer_index(system_instruction, user_message, verbose=False)
            docs_serialized = [
                {"score": float(doc_score), "metadata": doc.metadata, "content": doc.page_content, }
                for doc, doc_score in docs]
            ai_response = f"AI –æ—Ç–≤–µ—Ç: {ai_message}"

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é
            chat_history.append({'user': user_message, 'ai': ai_response})

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é 5 –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
            chat_history = chat_history[-5:]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –≤ —Å–µ—Å—Å–∏–∏
            request.session['chat_history'] = chat_history
            request.session.modified = True

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º JSON-–æ—Ç–≤–µ—Ç –¥–ª—è AJAX
            if request.headers.get('X-Requested-With') == 'XMLHttpRequest':
                return JsonResponse({
                    'user_message': user_message,
                    'ai_response': ai_response,
                    'current_docs': docs_serialized,
                })

        return render(request, self.template_name, {'chat_history': chat_history})


class ClearChatView(LoginRequiredMixin, View):
    def post(self, request, *args, **kwargs):
        # –û—á–∏—â–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞ –≤ —Å–µ—Å—Å–∏–∏
        if 'chat_history' in request.session:
            del request.session['chat_history']
            request.session.modified = True
        return redirect(reverse_lazy('chunks:ask_frida'))


class CurrentTestChunksView(LoginRequiredMixin, View):
    def get(self, *args, **kwargs):
        all_documents = None
        chunk_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chunk.pickle")
        with open(chunk_file, 'rb') as f:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º (–¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º) —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
            all_documents = pickle.load(f)
        context = {"all_documents": all_documents, }
        return render(request=self.request, template_name="app_chunks/current_documents.html", context=context)


class TestModelScoreView(LoginRequiredMixin, View):
    """—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤"""

    def get(self, *args, **kwargs):
        all_documents = None
        chunk_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chunk.pickle")
        with open(chunk_file, 'rb') as f:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º (–¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º) —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
            all_documents = pickle.load(f)
        all_urls = list(set(doc.metadata.get("url") for doc in all_documents))
        form = ModelScoreTestForm(all_urls=all_urls, initial={'urls': all_urls[:5]})
        context = {
            "form": form
        }
        return render(request=self.request, template_name="app_chunks/test_model_answer.html", context=context)

    def post(self, *args, **kwargs):
        all_documents = None
        chunk_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chunk.pickle")
        with open(chunk_file, 'rb') as f:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º (–¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º) —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
            all_documents = pickle.load(f)
        all_urls = list(set(doc.metadata.get("url") for doc in all_documents))
        form = ModelScoreTestForm(self.request.POST, all_urls=all_urls)
        if form.is_valid():
            test_urls = form.cleaned_data.get("urls")
            task = test_model_answer.delay(test_urls=test_urls)
            return render(self.request, "celery_task_progress.html", {
                "task_id": task.id,
                "task_name": f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏",
                "task_object_url": reverse_lazy("chunks:test_model_report"),
                "task_object_name": "FRIDA",
                "next_step_url": reverse_lazy("chunks:test_model_report"),
            })

        context = {
            "form": form
        }
        return render(request=self.request, template_name="app_chunks/test_model_answer.html", context=context)


class TestModelScoreReportView(LoginRequiredMixin, View):
    def get(self, *args, **kwargs):
        test_report = None
        all_documents = None
        chunk_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chunk.pickle")
        with open(chunk_file, 'rb') as f:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º (–¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º) —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
            all_documents = pickle.load(f)
        all_urls = list(doc.metadata.get("url") for doc in all_documents)
        all_urls_counts = Counter(all_urls)

        with open("test_report.json", encoding="utf-8") as f:
            test_report = json.load(f)
            for test_name, test_data in test_report.items():
                test_report[test_name]["chunks_counter"] = all_urls_counts.get(test_data.get("url"))
                test_report[test_name]["used_chunks"] = len(list(doc for doc in test_data.get("ai_documents", []) if
                                                                 doc.get("metadata", {}).get("url") == test_data.get(
                                                                     "url")))

        return render(self.request, 'app_chunks/test_model_results.html', {'tests': test_report})
