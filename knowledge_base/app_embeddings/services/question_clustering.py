import os
import django
import re
from typing import List, Dict, Tuple
from collections import defaultdict

import nltk
import numpy as np
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
import hdbscan
from nltk.corpus import stopwords
from sklearn.cluster import MiniBatchKMeans
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer

# Django init
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "knowledge_base.settings")
django.setup()

from app_chat.models import ChatMessage
from knowledge_base.settings import BASE_DIR

nltk.download("stopwords")
russian_stopwords = stopwords.words("russian")


class QuestionClusterer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é
    —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ HDBSCAN.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫—É FAISS –∏–Ω–¥–µ–∫—Å–∞, –∞–≤—Ç–æ–≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–≥–æ–≤,
    —ç–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.
    """

    def __init__(self, faiss_dir: str, model_name: str = "ai-forever/FRIDA"):
        """
        :param faiss_dir: –ü—É—Ç—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞.
        :param model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        """
        self.faiss_dir = faiss_dir
        self.model_name = model_name
        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)
        # self.clusterer = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean')
        self.clusterer = MiniBatchKMeans(n_clusters=10, batch_size=256, random_state=42)

        if not os.path.exists(self.faiss_dir):
            os.makedirs(self.faiss_dir)

        try:
            self.db = FAISS.load_local(self.faiss_dir, self.embeddings, index_name="index",
                                       allow_dangerous_deserialization=True)
            print("–ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è")
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.db = FAISS.from_documents([Document(page_content='', metadata={})], self.embeddings)
            print("–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è")


    def clean_text(self, text: str) -> str:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è."""
        text = text.lower()
        text = re.sub(r"[^\w\s]", "", text)
        return text.strip()

    def add_questions(self, raw_questions: List[Tuple[int, str]]):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ FAISS –∏–Ω–¥–µ–∫—Å.

        :param raw_questions: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (id, –≤–æ–ø—Ä–æ—Å)
        """
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        unique_q = {(qid, self.clean_text(q)) for qid, q in raw_questions if q.strip()}
        docs = [Document(page_content=q, metadata={"id": qid}) for qid, q in unique_q]
        self.db.add_documents(docs)
        self.db.save_local(folder_path=self.faiss_dir, index_name="index")

    def cluster_questions(self) -> Dict[int, List[Document]]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ FAISS.

        :return: –°–ª–æ–≤–∞—Ä—å {–∫–ª–∞—Å—Ç–µ—Ä_id: —Å–ø–∏—Å–æ–∫ Document}
        """
        docs = list(self.db.docstore._dict.values())

        if len(docs) < 5:
            return {-1: docs}

        questions = [doc.page_content for doc in docs]
        vectors = normalize(np.array(self.embeddings.embed_documents(questions)))

        labels = self.clusterer.fit_predict(vectors)

        clustered = defaultdict(list)
        for label, doc in zip(labels, docs):
            clustered[label].append(doc)

        return dict(clustered)

        return dict(clustered)

    def generate_tags(self, docs: List[Document], top_n: int = 5) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–≥–∏ (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞) –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å –ø–æ–º–æ—â—å—é TF-IDF.

        :param docs: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∞.
        :param top_n: –ö–æ–ª-–≤–æ —Ç–æ–ø-—Å–ª–æ–≤.
        :return: –°–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤.
        """
        texts = [doc.page_content for doc in docs]
        vectorizer = TfidfVectorizer(stop_words=russian_stopwords, max_features=1000)
        # vectorizer = TfidfVectorizer(max_features=1000)
        X = vectorizer.fit_transform(texts)
        feature_array = np.array(vectorizer.get_feature_names_out())

        tfidf_mean = np.asarray(X.mean(axis=0)).ravel()
        top_indices = tfidf_mean.argsort()[::-1][:top_n]
        return feature_array[top_indices].tolist()

    def export_json(self, clusters: Dict[int, List[Document]]) -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –≤ JSON –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏.

        :param clusters: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.
        :return: JSON-—Å—Ç—Ä–æ–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏.
        """
        import json
        import umap

        all_docs = [doc for docs in clusters.values() for doc in docs]
        questions = [doc.page_content for doc in all_docs]
        vectors = self.embeddings.embed_documents(questions)
        reducer = umap.UMAP(n_components=2, random_state=42)
        embedding_2d = reducer.fit_transform(vectors)

        # id –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        cluster_map = {}
        idx = 0
        for cluster_id, docs in clusters.items():
            for _ in docs:
                cluster_map[idx] = cluster_id
                idx += 1

        data = []
        for i, doc in enumerate(all_docs):
            data.append({
                "id": doc.metadata.get("id"),
                "text": doc.page_content,
                "cluster": int(cluster_map[i]),
                "x": float(embedding_2d[i, 0]),
                "y": float(embedding_2d[i, 1]),
            })

        return json.dumps(data, ensure_ascii=False)

    def print_clusters(self, clusters: Dict[int, List[Document]], max_examples: int = 5):
        """
        –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å.

        :param clusters: –°–ª–æ–≤–∞—Ä—å –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
        :param max_examples: –°–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞.
        """
        sorted_items = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)
        for cluster_id, docs in sorted_items:
            if cluster_id == -1:
                print(f"\n=== üìé –®—É–º (–Ω–µ –ø–æ–ø–∞–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã) ({len(docs)} –≤–æ–ø—Ä–æ—Å–æ–≤) ===")
            else:
                tags = self.generate_tags(docs)
                print(f"\n=== üß† –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({len(docs)} –≤–æ–ø—Ä–æ—Å–æ–≤), —Ç–µ–≥–∏: {', '.join(tags)} ===")
            for doc in docs[:max_examples]:
                print("  ‚Ä¢", doc.page_content)


if __name__ == "__main__":
    FAISS_DIR = os.path.join(BASE_DIR, "media", "user_questions_faiss")
    print(f"{FAISS_DIR=}")

    # –ë–µ—Ä—ë–º id –∏ —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –ë–î
    user_questions = ChatMessage.objects.filter(is_user=True).values_list('id', 'text')

    qc = QuestionClusterer(faiss_dir=FAISS_DIR)
    qc.add_questions(user_questions)
    clusters = qc.cluster_questions()
    qc.print_clusters(clusters)
